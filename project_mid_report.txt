================================================================================
ARGOVERSE 2 BEV GROUND TRUTH GENERATION - TECHNICAL REPORT
================================================================================

Project: MonoLayout Adaptation for Argoverse 2 Sensor Dataset
Date: November 19, 2025
Objective: Generate Bird's-Eye View (BEV) ground truth for road layout and 
          vehicle occupancy from monocular camera images

================================================================================
1. EXECUTIVE SUMMARY
================================================================================

This report documents the complete pipeline for generating BEV ground truth
from the Argoverse 2 Sensor Dataset for training monocular BEV prediction
models. The pipeline addresses critical challenges in coordinate system
transformations, camera-aware filtering, and multi-modal data synchronization.

Key Achievements:
- Developed camera-aware vehicle filtering (only includes visible vehicles)
- Implemented road-aware filtering (excludes off-road vehicles)
- Fixed coordinate system transformations for AV2 convention
- Achieved 100% coverage (BEV GT for every camera frame)
- Generated 638 visualizations for 2 sequences (31.9s each @ 10 FPS)

================================================================================
2. COORDINATE SYSTEMS AND TRANSFORMATIONS
================================================================================

2.1 ARGOVERSE 2 COORDINATE SYSTEM
----------------------------------
The Argoverse 2 dataset uses a right-handed coordinate system:

    Ego Vehicle Frame:
    - X-axis: Forward (vehicle's driving direction)
    - Y-axis: Left (perpendicular to driving direction)
    - Z-axis: Up (vertical, perpendicular to ground plane)
    
    Origin: Center of the ego vehicle's rear axle

Mathematical Representation:
    P_ego = [x, y, z]^T where:
    - x ∈ ℝ: forward distance (positive = ahead)
    - y ∈ ℝ: lateral distance (positive = left)
    - z ∈ ℝ: vertical distance (positive = up)

2.2 BIRD'S-EYE VIEW (BEV) IMAGE COORDINATES
--------------------------------------------
BEV images are 2D raster representations viewed from above:

    BEV Image Frame:
    - X-axis (horizontal): Left to Right
    - Y-axis (vertical): Far to Near (top to bottom)
    - Origin: Top-left corner of image
    
    Dimensions: 256 × 256 pixels
    Spatial Coverage: 40m forward × 40m lateral (±20m)
    Resolution: 40m / 256 = 0.15625 m/pixel

Coordinate Transformation (Ego → BEV):
    
    Given ego coordinates (x_ego, y_ego) and BEV parameters:
        range = 40.0  # meters
        size = 256    # pixels
        res = range / size = 0.15625  # m/pixel
    
    BEV pixel coordinates:
        u_bev = (y_ego + range/2) / res
        v_bev = (range - x_ego) / res
    
    Where:
        u_bev ∈ [0, 255]: horizontal pixel coordinate
        v_bev ∈ [0, 255]: vertical pixel coordinate

Mapping Explanation:
    1. y_ego (left) → u_bev (horizontal): Direct mapping with offset
       - y_ego = -20m → u_bev = 0 (left edge)
       - y_ego = 0m → u_bev = 128 (center)
       - y_ego = +20m → u_bev = 255 (right edge)
    
    2. x_ego (forward) → v_bev (vertical, inverted): Inverted mapping
       - x_ego = 40m → v_bev = 0 (top, far)
       - x_ego = 20m → v_bev = 128 (middle)
       - x_ego = 0m → v_bev = 255 (bottom, near)

2.3 CAMERA COORDINATE SYSTEM
-----------------------------
The pinhole camera model projects 3D points to 2D image plane:

    Camera Frame:
    - X-axis: Right
    - Y-axis: Down
    - Z-axis: Forward (optical axis)
    - Origin: Camera optical center

Projection Model:
    Given 3D point P_cam = [X_cam, Y_cam, Z_cam]^T in camera frame,
    the 2D image coordinates are:
    
    [u]   [f_x  0   c_x] [X_cam / Z_cam]
    [v] = [0    f_y c_y] [Y_cam / Z_cam]
    [1]   [0    0   1  ] [1            ]
    
    Where:
        K = intrinsic matrix
        f_x, f_y = focal lengths (pixels)
        c_x, c_y = principal point (pixels)
        u, v = image pixel coordinates

2.4 TRANSFORMATION PIPELINE
----------------------------
Complete transformation from city frame to image:

    P_city → P_ego → P_cam → p_image
    
    Step 1: City to Ego (SE(3) transformation)
        P_ego = SE3_ego_city^(-1) · P_city
        
        Where SE3 is a 4×4 transformation matrix:
            [R  t]
            [0  1]
        
        R ∈ SO(3): 3×3 rotation matrix
        t ∈ ℝ³: translation vector

    Step 2: Ego to Camera (extrinsic transformation)
        P_cam = R_cam_ego · P_ego + t_cam_ego
        
        Provided by AV2 API via pinhole camera calibration

    Step 3: Camera to Image (intrinsic projection)
        p_image = K · [P_cam / Z_cam]
        
        Handled by: pinhole_camera.project_ego_to_img()

CRITICAL DISCOVERY:
    Argoverse 2 annotations are ALREADY in ego frame!
    - No city→ego transformation needed for annotations
    - Direct use: P_ego = [tx_m, ty_m, tz_m]^T from annotation

================================================================================
3. ROAD BEV GENERATION
================================================================================

3.1 VECTOR MAP REPRESENTATION
------------------------------
Argoverse 2 provides high-definition vector maps:

    Map Elements:
    - Lane segments: Polylines representing drivable lanes
    - Lane boundaries: Left/right edges of lanes
    - Crosswalks: Pedestrian crossing areas
    - Drivable areas: General drivable regions

Data Structure:
    Lane Segment = {
        id: unique identifier,
        left_boundary: List[Point3D],
        right_boundary: List[Point3D],
        successors: List[lane_id],
        predecessors: List[lane_id]
    }
    
    Point3D = (x, y, z) in city coordinate frame

3.2 RASTERIZATION ALGORITHM
----------------------------
Convert vector map to BEV raster image:

Input:
    - Vector map M = {L₁, L₂, ..., Lₙ} (lane segments)
    - Ego pose SE3_ego_city at timestamp t
    - BEV parameters: range, size

Algorithm:
    1. Initialize BEV image: I_bev ← zeros(size, size)
    
    2. For each lane segment Lᵢ ∈ M:
        a. Extract boundary points in city frame:
           P_city = {p₁, p₂, ..., pₘ}
        
        b. Transform to ego frame:
           P_ego = SE3_ego_city^(-1) · P_city
        
        c. Filter points within range:
           P_filtered = {p ∈ P_ego | 0 ≤ p.x ≤ 40 ∧ -20 ≤ p.y ≤ 20}
        
        d. Convert to BEV pixel coordinates:
           P_bev = [(p.y + 20)/res, (40 - p.x)/res] for p in P_filtered
        
        e. Rasterize polygon:
           cv2.fillPoly(I_bev, [P_bev], 255)
    
    3. Return I_bev

Complexity: O(n · m) where n = number of lanes, m = points per lane

3.3 SPATIAL FILTERING
----------------------
Efficient filtering using bounding box:

    For point p in ego frame:
        include(p) = (0 ≤ p.x ≤ 40) ∧ (-20 ≤ p.y ≤ 20)
    
    Geometric interpretation:
        - Forward range: [0, 40] meters
        - Lateral range: [-20, 20] meters
        - Total coverage: 40m × 40m rectangle ahead of ego

================================================================================
4. VEHICLE BEV GENERATION
================================================================================

4.1 3D BOUNDING BOX REPRESENTATION
-----------------------------------
Vehicles are represented as oriented 3D cuboids:

    Cuboid Parameters:
    - Center: (tx, ty, tz) in ego frame
    - Dimensions: (length, width, height) in meters
    - Orientation: quaternion (qw, qx, qy, qz)

Argoverse 2 Convention:
    - length: extent along vehicle's longitudinal axis
    - width: extent along vehicle's lateral axis
    - height: extent along vehicle's vertical axis

4.2 BOUNDING BOX CORNER CALCULATION
------------------------------------
Compute 8 corners of 3D bounding box:

Given:
    center = [cx, cy, cz]^T
    dims = [l, w, h]^T
    R = rotation matrix from quaternion

Corner offsets in vehicle frame:
    Δ = [±l/2, ±w/2, ±h/2]^T (8 combinations)

Corners in ego frame:
    corners = R · Δ + center

For BEV (2D projection), we only need 4 corners:
    corners_2d = [
        [cx - l/2, cy - w/2],  # back-right
        [cx + l/2, cy - w/2],  # front-right
        [cx + l/2, cy + w/2],  # front-left
        [cx - l/2, cy + w/2]   # back-left
    ]

CRITICAL FIX:
    Original code incorrectly swapped length/width
    Correct mapping: X-axis (forward) = length, Y-axis (left) = width

4.3 CAMERA VISIBILITY FILTERING
--------------------------------
Only include vehicles visible in camera field of view:

Visibility Check:
    1. Project vehicle center to image:
       uv, _, is_valid = pinhole_camera.project_ego_to_img(P_vehicle)
    
    2. Check validity:
       visible = is_valid[0] ∧ (0 ≤ u < width) ∧ (0 ≤ v < height)
    
    3. Include vehicle only if visible = True

Mathematical Basis:
    The is_valid flag checks:
    - Z_cam > 0 (point in front of camera)
    - Point within camera frustum (FOV constraints)
    - Valid projection (no numerical issues)

Projection Equation:
    [u]       [X_cam]
    [v] = K · [Y_cam] / Z_cam
    [1]       [Z_cam]
    
    Valid if: Z_cam > ε (typically ε = 0.1m)

4.4 ROAD OVERLAP FILTERING
---------------------------
Exclude vehicles not on drivable road:

Algorithm:
    Input: vehicle position (x_ego, y_ego), road BEV I_road
    
    1. Convert to BEV coordinates:
       u = (y_ego + 20) / res
       v = (40 - x_ego) / res
    
    2. Check neighborhood with radius r = 3 pixels:
       on_road = ∃(du, dv) ∈ [-r, r]² : I_road[v+dv, u+du] > 0
    
    3. Return on_road

Tolerance Radius:
    r = 3 pixels ≈ 0.47 meters
    Accounts for:
    - Annotation uncertainty
    - Vehicle size (center may be slightly off lane)
    - Rasterization discretization

4.5 COMPLETE VEHICLE FILTERING PIPELINE
----------------------------------------
Multi-stage filtering ensures high-quality ground truth:

    For each vehicle V with position P_v:
        
        Stage 1: Spatial Range
            if ¬(0 ≤ P_v.x ≤ 40 ∧ -20 ≤ P_v.y ≤ 20):
                REJECT
        
        Stage 2: Camera Visibility
            uv, _, valid = project(P_v)
            if ¬valid[0]:
                REJECT
        
        Stage 3: Road Overlap
            if ¬check_road_overlap(P_v, I_road):
                REJECT
        
        Stage 4: Rasterization
            corners = compute_corners(V)
            corners_bev = transform_to_bev(corners)
            cv2.fillPoly(I_vehicle, [corners_bev], 255)

Result:
    - Only vehicles visible in camera AND on roads are included
    - Prevents training on invalid/occluded vehicles
    - Improves model generalization

================================================================================
5. TIMESTAMP SYNCHRONIZATION
================================================================================

5.1 MULTI-MODAL TEMPORAL ALIGNMENT
-----------------------------------
Argoverse 2 sensors operate at different frequencies:

    Sensor Frequencies:
    - Camera (ring_front_center): ~10 Hz
    - LiDAR: 10 Hz
    - Annotations: Aligned with LiDAR timestamps

Challenge:
    Camera and LiDAR timestamps are not perfectly synchronized

5.2 NEAREST NEIGHBOR TEMPORAL MATCHING
---------------------------------------
Find closest annotation for each camera frame:

Algorithm:
    Input: 
        T_cam = {t₁, t₂, ..., tₙ} (camera timestamps)
        T_lidar = {s₁, s₂, ..., sₘ} (LiDAR/annotation timestamps)
    
    For each t ∈ T_cam:
        1. Find closest LiDAR timestamp:
           s* = argmin_{s ∈ T_lidar} |t - s|
        
        2. Compute time difference:
           Δt = |t - s*| / 10⁶  (convert ns to ms)
        
        3. Accept if within threshold:
           if Δt ≤ 100 ms:
               use annotations at s*
           else:
               skip (no valid annotations)

Threshold Selection:
    τ = 100 ms chosen because:
    - Sensor frequency: 10 Hz → 100 ms period
    - Vehicle speed: ~30 m/s → 3m displacement in 100ms
    - Acceptable for BEV resolution: 0.15625 m/pixel

CRITICAL BUG FIX:
    Original: Loop over annotation timestamps → missed camera frames
    Fixed: Loop over camera timestamps → 100% coverage

5.3 TEMPORAL CONSISTENCY
-------------------------
Ensure ego pose consistency:

    For camera timestamp t_cam with matched annotation timestamp t_lidar:
        - Use ego pose at t_lidar (not t_cam)
        - Ensures vehicle positions and ego pose are consistent
        - Prevents coordinate frame misalignment

================================================================================
6. EGO MOTION VISUALIZATION
================================================================================

6.1 MOTION VECTOR COMPUTATION
------------------------------
Calculate ego vehicle movement between consecutive frames:

Given:
    - Current pose: SE3_curr at time t
    - Previous pose: SE3_prev at time t-1

Step 1: Transform previous position to current ego frame
    P_prev_city = SE3_prev · [0, 0, 0, 1]^T  (previous ego in city)
    P_prev_curr = SE3_curr^(-1) · P_prev_city  (in current ego frame)

Step 2: Compute movement vector
    Δ = [0, 0, 0]^T - P_prev_curr[:2]  (movement in ego frame)
    
    Where:
        Δ.x: forward/backward movement
        Δ.y: left/right movement

Step 3: Normalize for visualization
    Δ_norm = Δ / ||Δ|| · arrow_length

CRITICAL FIX:
    Original: Transformed movement vector (incorrect)
    Fixed: Transform positions, then compute movement

6.2 ARROW RENDERING IN BEV
---------------------------
Map ego motion to BEV image coordinates:

    Given movement Δ = [Δx, Δy] in ego frame:
    
    Arrow start: (center_x, center_y) = (128, 128)  # BEV center
    
    Arrow end:
        end_x = center_x - Δy · length  # Y (left) → X (left)
        end_y = center_y - Δx · length  # X (forward) → Y (up)
    
    Render:
        cv2.arrowedLine(I_bev, (start_x, start_y), (end_x, end_y), 
                       color=(255, 255, 0), thickness=3)

Coordinate Mapping:
    Ego X (forward) → BEV -Y (upward in image)
    Ego Y (left) → BEV -X (leftward in image)

================================================================================
7. VISUALIZATION PIPELINE
================================================================================

7.1 COLOR-CODED BOUNDING BOXES
-------------------------------
Visual encoding of vehicle status:

    Color Scheme:
        Green (0, 255, 0):  Vehicle on road in BEV range
        Orange (0, 165, 255): Vehicle off road in BEV range
        Red (0, 0, 255):    Vehicle outside BEV range

Decision Tree:
    if in_bev_range:
        if on_road:
            color = GREEN
        else:
            color = ORANGE
    else:
        color = RED

Visual Properties:
    - Marker radius: 8-12 pixels (based on status)
    - Box thickness: 2-3 pixels (based on status)
    - Label: "V{id}:{distance}m(status)"

7.2 FOUR-PANEL LAYOUT
----------------------
Comprehensive visualization combining multiple views:

    Layout (800×800 pixels):
        ┌─────────────┬─────────────┐
        │   Camera    │ Combined BEV│
        │   + Boxes   │ Road+Vehicle│
        │  (400×400)  │  (400×400)  │
        ├─────────────┼─────────────┤
        │  Road BEV   │ Vehicle BEV │
        │   GT Only   │   GT Only   │
        │  (400×400)  │  (400×400)  │
        └─────────────┴─────────────┘

Panel Details:
    1. Top-Left: Camera image with projected bounding boxes
    2. Top-Right: Combined BEV (road + vehicles + ego arrow)
    3. Bottom-Left: Road BEV ground truth (green)
    4. Bottom-Right: Vehicle BEV ground truth (red)

7.3 VIDEO GENERATION
--------------------
Create temporal sequences for analysis:

    Video Parameters:
        - Resolution: 800×800 pixels
        - Frame rate: 10 FPS
        - Codec: MP4V
        - Duration: 31.9 seconds (319 frames)

    Encoding:
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(path, fourcc, fps, (width, height))

================================================================================
8. IMPLEMENTATION DETAILS
================================================================================

8.1 SOFTWARE ARCHITECTURE
--------------------------
Modular design with clear separation of concerns:

    generate_groundtruth_av2.py
        ├── ArgoverseV2LayoutGenerator
        │   ├── __init__(args)
        │   ├── generate_road_bev(log_id, output_dir)
        │   ├── generate_vehicle_bev(log_id, output_dir)
        │   └── world_to_bev(points_3d) → points_2d
        │
    visualize_av2_bev.py
        ├── check_vehicle_on_road(pos, road_bev) → bool
        ├── draw_bounding_boxes_on_camera(img, log_id, ts, path)
        └── create_av2_visualization(paths...) → combined_image

8.2 ARGOVERSE 2 API USAGE
--------------------------
Leveraging official API for robustness:

    Key Components:
        1. AV2SensorDataLoader
           - Loads sensor data and annotations
           - Provides ego poses and calibrations
        
        2. PinholeCamera
           - Encapsulates camera intrinsics/extrinsics
           - project_ego_to_img(points) → (uv, valid)
        
        3. ArgoverseStaticMap
           - Provides vector map data
           - get_scenario_lane_segments() → lane_list

    Benefits:
        - Handles coordinate transformations internally
        - Manages data I/O efficiently (Feather format)
        - Provides validated calibrations

8.3 PERFORMANCE OPTIMIZATIONS
------------------------------
Efficient processing for large-scale datasets:

    1. Vectorized Operations
       - NumPy array operations for point transformations
       - Batch processing of lane segments
    
    2. Spatial Indexing
       - Pre-filter map elements by distance
       - Avoid processing distant lanes
    
    3. Caching
       - Load pinhole camera once per log
       - Reuse road BEV for vehicle filtering
    
    4. I/O Optimization
       - Use Feather format (faster than CSV)
       - Parallel processing capability (multi-log)

Processing Speed:
    - Road BEV: ~35 frames/second
    - Vehicle BEV: ~85 frames/second
    - Visualization: ~10 frames/second

================================================================================
9. VALIDATION AND QUALITY ASSURANCE
================================================================================

9.1 QUANTITATIVE METRICS
-------------------------
Measured on 2 sequences (638 frames total):

    Coverage Metrics:
        - Road BEV: 638/638 (100%)
        - Vehicle BEV: 632/638 (99.1%)
        - Missing: 6 frames with no valid vehicles
    
    Vehicle Statistics (per frame average):
        - Total vehicles detected: ~13
        - Vehicles in BEV range: ~6
        - Vehicles on road: ~4-5
        - Vehicles visible in camera: ~4-5
    
    Spatial Accuracy:
        - Vehicle-road overlap: 44.38%
        - Center distance: 10.0 pixels (1.56m)

9.2 VISUAL INSPECTION
----------------------
Manual verification of visualizations:

    Checklist:
        ✓ Bounding boxes align with vehicles in camera
        ✓ BEV vehicles correspond to camera detections
        ✓ Road layout matches visible lanes
        ✓ Ego arrow points in driving direction
        ✓ Color coding correctly reflects vehicle status
        ✓ No vehicles floating off-road in BEV

9.3 EDGE CASES HANDLED
-----------------------
Robust handling of challenging scenarios:

    1. No Vehicles in Frame
       - Generate empty vehicle BEV (all zeros)
       - Road BEV still generated normally
    
    2. Vehicles Outside Camera FOV
       - Excluded from vehicle BEV
       - Prevents training on invisible objects
    
    3. Timestamp Mismatch
       - Skip if >100ms difference
       - Prevents temporal inconsistency
    
    4. Off-Road Vehicles
       - Excluded from vehicle BEV
       - Shown in orange in visualization

================================================================================
10. COMPARISON WITH ARGOVERSE 1 PREPROCESSING
================================================================================

10.1 KEY DIFFERENCES
--------------------

Aspect                  | Argoverse 1          | Argoverse 2
------------------------|----------------------|-------------------------
Coordinate System       | +X=Right, +Y=Forward | +X=Forward, +Y=Left
Annotation Frame        | City frame           | Ego frame (already!)
Map Format              | Rasterized HD maps   | Vector maps
API Support             | Limited              | Comprehensive AV2 API
Camera Projection       | Manual calculation   | PinholeCamera.project()
Vehicle Filtering       | Spatial only         | Spatial + Camera + Road
Timestamp Sync          | Direct matching      | Nearest neighbor (100ms)
Data Format             | JSON                 | Feather (faster)

10.2 IMPROVEMENTS MADE
-----------------------

    1. Camera-Aware Filtering
       - Original: Includes all vehicles in spatial range
       - Improved: Only vehicles visible in camera
       - Impact: Better alignment with monocular prediction task
    
    2. Road-Aware Filtering
       - Original: Includes vehicles anywhere in range
       - Improved: Only vehicles on drivable roads
       - Impact: Reduces false positives from parking lots
    
    3. Coordinate System Handling
       - Original: Manual transformation matrices
       - Improved: Leverages AV2 API transformations
       - Impact: Fewer bugs, more robust
    
    4. Temporal Coverage
       - Original: Missed some frames due to timestamp mismatch
       - Improved: 100% coverage by looping over camera timestamps
       - Impact: More training data, better temporal consistency

================================================================================
11. MATHEMATICAL SUMMARY
================================================================================

11.1 COMPLETE TRANSFORMATION CHAIN
-----------------------------------

City Frame → Ego Frame → BEV Image:

    P_city ∈ ℝ³ (city coordinates)
    ↓ SE3_ego_city^(-1)
    P_ego = [x, y, z]^T ∈ ℝ³ (ego coordinates)
    ↓ Spatial filter: 0≤x≤40, -20≤y≤20
    P_filtered ∈ ℝ³
    ↓ BEV projection: (x,y) → (u,v)
    P_bev = [u, v]^T ∈ ℤ² (pixel coordinates)

Where:
    u = ⌊(y + 20) / 0.15625⌋
    v = ⌊(40 - x) / 0.15625⌋

11.2 CAMERA PROJECTION
----------------------

Ego Frame → Camera Frame → Image:

    P_ego ∈ ℝ³
    ↓ Extrinsic transformation
    P_cam = R_cam_ego · P_ego + t_cam_ego ∈ ℝ³
    ↓ Perspective projection
    p_norm = [X_cam/Z_cam, Y_cam/Z_cam, 1]^T ∈ ℝ³
    ↓ Intrinsic transformation
    p_image = K · p_norm ∈ ℝ³

Where:
    K = [f_x  0   c_x]
        [0    f_y c_y]
        [0    0   1  ]

Validity: Z_cam > 0 ∧ 0 ≤ u < width ∧ 0 ≤ v < height

11.3 FILTERING LOGIC
--------------------

Boolean algebra for vehicle inclusion:

    include(V) = spatial(V) ∧ visible(V) ∧ on_road(V)

Where:
    spatial(V) = (0 ≤ V.x ≤ 40) ∧ (-20 ≤ V.y ≤ 20)
    
    visible(V) = is_valid(project(V.position))
    
    on_road(V) = ∃(du,dv) ∈ [-3,3]² : 
                 I_road[bev_v(V)+dv, bev_u(V)+du] > 0

================================================================================
12. FUTURE WORK AND EXTENSIONS
================================================================================

12.1 POTENTIAL IMPROVEMENTS
----------------------------

    1. Multi-Camera Support
       - Extend to all 7 ring cameras
       - 360° BEV coverage
       - Requires coordinate frame alignment
    
    2. Temporal Consistency
       - Track vehicles across frames
       - Smooth trajectory interpolation
       - Handle occlusions better
    
    3. Semantic Segmentation
       - Separate vehicle classes (car, truck, bus)
       - Lane-level road segmentation
       - Crosswalk and intersection marking
    
    4. Data Augmentation
       - Random BEV rotations
       - Scale variations
       - Weather/lighting augmentation

12.2 SCALABILITY
----------------

    Current: 2 sequences (638 frames) in ~5 minutes
    Full Dataset: 240 sequences (~76,560 frames)
    
    Estimated Time:
        - Road BEV: 76,560 / 35 fps ≈ 36 minutes
        - Vehicle BEV: 76,560 / 85 fps ≈ 15 minutes
        - Total: ~1 hour for full dataset
    
    Parallelization:
        - Process logs independently
        - Multi-GPU for visualization
        - Distributed processing for large-scale

================================================================================
13. CONCLUSION
================================================================================

This project successfully adapted the MonoLayout BEV ground truth generation
pipeline for the Argoverse 2 Sensor Dataset. Key achievements include:

1. Correct handling of Argoverse 2 coordinate conventions
2. Camera-aware and road-aware vehicle filtering
3. Robust timestamp synchronization
4. Comprehensive visualization for quality assurance
5. 100% frame coverage with high-quality ground truth

The generated BEV ground truth is now ready for training monocular BEV
prediction models, with improved quality compared to naive spatial filtering
approaches.

Technical Contributions:
- Mathematical formalization of coordinate transformations
- Multi-stage filtering pipeline for vehicle selection
- Temporal synchronization algorithm
- Comprehensive validation methodology

The pipeline is production-ready and can be scaled to the full Argoverse 2
dataset for large-scale model training.

================================================================================
END OF REPORT
================================================================================
